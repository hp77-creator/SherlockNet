{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:05.424338Z",
     "iopub.status.busy": "2021-12-10T03:50:05.42395Z",
     "iopub.status.idle": "2021-12-10T03:50:05.430958Z",
     "shell.execute_reply": "2021-12-10T03:50:05.430065Z",
     "shell.execute_reply.started": "2021-12-10T03:50:05.424293Z"
    },
    "id": "ro9B5yFIU6r3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:05.433965Z",
     "iopub.status.busy": "2021-12-10T03:50:05.43288Z",
     "iopub.status.idle": "2021-12-10T03:50:05.447898Z",
     "shell.execute_reply": "2021-12-10T03:50:05.447083Z",
     "shell.execute_reply.started": "2021-12-10T03:50:05.433912Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:05.449435Z",
     "iopub.status.busy": "2021-12-10T03:50:05.448914Z",
     "iopub.status.idle": "2021-12-10T03:50:09.752246Z",
     "shell.execute_reply": "2021-12-10T03:50:09.751394Z",
     "shell.execute_reply.started": "2021-12-10T03:50:05.449398Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\allen\\Desktop\\D major\\digit recognization\\train.csv\")\n",
    "X_test = pd.read_csv(r\"C:\\Users\\allen\\Desktop\\D major\\digit recognization\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:09.771845Z",
     "iopub.status.busy": "2021-12-10T03:50:09.771605Z",
     "iopub.status.idle": "2021-12-10T03:50:09.778082Z",
     "shell.execute_reply": "2021-12-10T03:50:09.777265Z",
     "shell.execute_reply.started": "2021-12-10T03:50:09.771816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:09.780603Z",
     "iopub.status.busy": "2021-12-10T03:50:09.779586Z",
     "iopub.status.idle": "2021-12-10T03:50:10.223501Z",
     "shell.execute_reply": "2021-12-10T03:50:10.222679Z",
     "shell.execute_reply.started": "2021-12-10T03:50:09.780554Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, y_train = train.iloc[:, 1:].values , train.iloc[:, 0].values\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = .25)\n",
    "x_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.225087Z",
     "iopub.status.busy": "2021-12-10T03:50:10.224784Z",
     "iopub.status.idle": "2021-12-10T03:50:10.23149Z",
     "shell.execute_reply": "2021-12-10T03:50:10.230481Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.225046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31500, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.233244Z",
     "iopub.status.busy": "2021-12-10T03:50:10.232934Z",
     "iopub.status.idle": "2021-12-10T03:50:10.242763Z",
     "shell.execute_reply": "2021-12-10T03:50:10.24195Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.233202Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotting(x,y,index):\n",
    "    plt.imshow(x[index])\n",
    "    plt.xlabel(y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.244816Z",
     "iopub.status.busy": "2021-12-10T03:50:10.244291Z",
     "iopub.status.idle": "2021-12-10T03:50:10.405695Z",
     "shell.execute_reply": "2021-12-10T03:50:10.405093Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.24476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting(x_train.reshape((31500,28,28)), y_train, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.409678Z",
     "iopub.status.busy": "2021-12-10T03:50:10.409192Z",
     "iopub.status.idle": "2021-12-10T03:50:10.414822Z",
     "shell.execute_reply": "2021-12-10T03:50:10.413941Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.409631Z"
    },
    "id": "BkRv4kThW5bK"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "epochs = 5\n",
    "n_classes = 10\n",
    "in_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.416241Z",
     "iopub.status.busy": "2021-12-10T03:50:10.416018Z",
     "iopub.status.idle": "2021-12-10T03:50:10.62441Z",
     "shell.execute_reply": "2021-12-10T03:50:10.62366Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.416214Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_val = x_val/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.626327Z",
     "iopub.status.busy": "2021-12-10T03:50:10.625849Z",
     "iopub.status.idle": "2021-12-10T03:50:10.630816Z",
     "shell.execute_reply": "2021-12-10T03:50:10.63013Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.62629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1,28,28)\n",
    "x_val = x_val.reshape(-1,28,28)\n",
    "x_test = x_test.reshape(-1,28,28)\n",
    "(256,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.632595Z",
     "iopub.status.busy": "2021-12-10T03:50:10.632168Z",
     "iopub.status.idle": "2021-12-10T03:50:10.735171Z",
     "shell.execute_reply": "2021-12-10T03:50:10.734511Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.632563Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).type(torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.from_numpy(x_val).type(torch.float32).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test).type(torch.float32).unsqueeze(1)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_val = torch.from_numpy(y_val)\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "val_ds = TensorDataset(x_val, y_val)\n",
    "train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.73692Z",
     "iopub.status.busy": "2021-12-10T03:50:10.736508Z",
     "iopub.status.idle": "2021-12-10T03:50:10.74715Z",
     "shell.execute_reply": "2021-12-10T03:50:10.746295Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.736889Z"
    },
    "id": "F8FFL2uzXFQO"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels =1, out_channels =6, kernel_size = (3,3), stride=(1,1), padding=(1,1))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels =16, kernel_size = (3,3), stride=(1,1), padding=(1,1))\n",
    "        self.fc1 = nn.Linear(16*7*7, 120)\n",
    "\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = x.view(-1, 16*7*7)\n",
    "        #print( x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.748865Z",
     "iopub.status.busy": "2021-12-10T03:50:10.748321Z",
     "iopub.status.idle": "2021-12-10T03:50:10.767628Z",
     "shell.execute_reply": "2021-12-10T03:50:10.766746Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.748825Z"
    },
    "id": "CFzBgMjCXH0s"
   },
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.769934Z",
     "iopub.status.busy": "2021-12-10T03:50:10.769288Z",
     "iopub.status.idle": "2021-12-10T03:50:10.780779Z",
     "shell.execute_reply": "2021-12-10T03:50:10.77995Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.769879Z"
    },
    "id": "j9dyv0tnXK6z"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.782567Z",
     "iopub.status.busy": "2021-12-10T03:50:10.78223Z",
     "iopub.status.idle": "2021-12-10T03:50:28.931912Z",
     "shell.execute_reply": "2021-12-10T03:50:28.930871Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.78252Z"
    },
    "id": "_sG_9wweXNFR",
    "outputId": "7cd6cc50-2b1a-4fdf-8388-ba850a1f134c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/124], Loss: 2.2971\n",
      "Epoch [1/5], Step [2/124], Loss: 2.2968\n",
      "Epoch [1/5], Step [3/124], Loss: 2.2977\n",
      "Epoch [1/5], Step [4/124], Loss: 2.2915\n",
      "Epoch [1/5], Step [5/124], Loss: 2.2754\n",
      "Epoch [1/5], Step [6/124], Loss: 2.2827\n",
      "Epoch [1/5], Step [7/124], Loss: 2.2615\n",
      "Epoch [1/5], Step [8/124], Loss: 2.2482\n",
      "Epoch [1/5], Step [9/124], Loss: 2.2443\n",
      "Epoch [1/5], Step [10/124], Loss: 2.2377\n",
      "Epoch [1/5], Step [11/124], Loss: 2.2189\n",
      "Epoch [1/5], Step [12/124], Loss: 2.2078\n",
      "Epoch [1/5], Step [13/124], Loss: 2.2014\n",
      "Epoch [1/5], Step [14/124], Loss: 2.1818\n",
      "Epoch [1/5], Step [15/124], Loss: 2.1573\n",
      "Epoch [1/5], Step [16/124], Loss: 2.1113\n",
      "Epoch [1/5], Step [17/124], Loss: 2.1205\n",
      "Epoch [1/5], Step [18/124], Loss: 2.1099\n",
      "Epoch [1/5], Step [19/124], Loss: 2.0675\n",
      "Epoch [1/5], Step [20/124], Loss: 2.0090\n",
      "Epoch [1/5], Step [21/124], Loss: 1.9728\n",
      "Epoch [1/5], Step [22/124], Loss: 1.9642\n",
      "Epoch [1/5], Step [23/124], Loss: 1.9129\n",
      "Epoch [1/5], Step [24/124], Loss: 1.8744\n",
      "Epoch [1/5], Step [25/124], Loss: 1.7761\n",
      "Epoch [1/5], Step [26/124], Loss: 1.7830\n",
      "Epoch [1/5], Step [27/124], Loss: 1.7450\n",
      "Epoch [1/5], Step [28/124], Loss: 1.6673\n",
      "Epoch [1/5], Step [29/124], Loss: 1.5756\n",
      "Epoch [1/5], Step [30/124], Loss: 1.5319\n",
      "Epoch [1/5], Step [31/124], Loss: 1.4949\n",
      "Epoch [1/5], Step [32/124], Loss: 1.4485\n",
      "Epoch [1/5], Step [33/124], Loss: 1.3554\n",
      "Epoch [1/5], Step [34/124], Loss: 1.3510\n",
      "Epoch [1/5], Step [35/124], Loss: 1.2853\n",
      "Epoch [1/5], Step [36/124], Loss: 1.2615\n",
      "Epoch [1/5], Step [37/124], Loss: 1.2234\n",
      "Epoch [1/5], Step [38/124], Loss: 1.0879\n",
      "Epoch [1/5], Step [39/124], Loss: 1.0309\n",
      "Epoch [1/5], Step [40/124], Loss: 1.0018\n",
      "Epoch [1/5], Step [41/124], Loss: 1.0458\n",
      "Epoch [1/5], Step [42/124], Loss: 0.9615\n",
      "Epoch [1/5], Step [43/124], Loss: 0.9986\n",
      "Epoch [1/5], Step [44/124], Loss: 0.8732\n",
      "Epoch [1/5], Step [45/124], Loss: 0.8752\n",
      "Epoch [1/5], Step [46/124], Loss: 0.8755\n",
      "Epoch [1/5], Step [47/124], Loss: 0.8589\n",
      "Epoch [1/5], Step [48/124], Loss: 0.8347\n",
      "Epoch [1/5], Step [49/124], Loss: 0.8303\n",
      "Epoch [1/5], Step [50/124], Loss: 0.8278\n",
      "Epoch [1/5], Step [51/124], Loss: 0.7839\n",
      "Epoch [1/5], Step [52/124], Loss: 0.7545\n",
      "Epoch [1/5], Step [53/124], Loss: 0.7635\n",
      "Epoch [1/5], Step [54/124], Loss: 0.6823\n",
      "Epoch [1/5], Step [55/124], Loss: 0.6784\n",
      "Epoch [1/5], Step [56/124], Loss: 0.6884\n",
      "Epoch [1/5], Step [57/124], Loss: 0.7442\n",
      "Epoch [1/5], Step [58/124], Loss: 0.5548\n",
      "Epoch [1/5], Step [59/124], Loss: 0.6977\n",
      "Epoch [1/5], Step [60/124], Loss: 0.7627\n",
      "Epoch [1/5], Step [61/124], Loss: 0.5548\n",
      "Epoch [1/5], Step [62/124], Loss: 0.6160\n",
      "Epoch [1/5], Step [63/124], Loss: 0.5297\n",
      "Epoch [1/5], Step [64/124], Loss: 0.6124\n",
      "Epoch [1/5], Step [65/124], Loss: 0.5873\n",
      "Epoch [1/5], Step [66/124], Loss: 0.6122\n",
      "Epoch [1/5], Step [67/124], Loss: 0.6273\n",
      "Epoch [1/5], Step [68/124], Loss: 0.5812\n",
      "Epoch [1/5], Step [69/124], Loss: 0.5989\n",
      "Epoch [1/5], Step [70/124], Loss: 0.5963\n",
      "Epoch [1/5], Step [71/124], Loss: 0.5591\n",
      "Epoch [1/5], Step [72/124], Loss: 0.5652\n",
      "Epoch [1/5], Step [73/124], Loss: 0.5090\n",
      "Epoch [1/5], Step [74/124], Loss: 0.6116\n",
      "Epoch [1/5], Step [75/124], Loss: 0.4820\n",
      "Epoch [1/5], Step [76/124], Loss: 0.5877\n",
      "Epoch [1/5], Step [77/124], Loss: 0.4839\n",
      "Epoch [1/5], Step [78/124], Loss: 0.4653\n",
      "Epoch [1/5], Step [79/124], Loss: 0.4693\n",
      "Epoch [1/5], Step [80/124], Loss: 0.5177\n",
      "Epoch [1/5], Step [81/124], Loss: 0.5920\n",
      "Epoch [1/5], Step [82/124], Loss: 0.4405\n",
      "Epoch [1/5], Step [83/124], Loss: 0.5602\n",
      "Epoch [1/5], Step [84/124], Loss: 0.4611\n",
      "Epoch [1/5], Step [85/124], Loss: 0.5176\n",
      "Epoch [1/5], Step [86/124], Loss: 0.4876\n",
      "Epoch [1/5], Step [87/124], Loss: 0.5260\n",
      "Epoch [1/5], Step [88/124], Loss: 0.4474\n",
      "Epoch [1/5], Step [89/124], Loss: 0.4410\n",
      "Epoch [1/5], Step [90/124], Loss: 0.4285\n",
      "Epoch [1/5], Step [91/124], Loss: 0.4380\n",
      "Epoch [1/5], Step [92/124], Loss: 0.4797\n",
      "Epoch [1/5], Step [93/124], Loss: 0.4660\n",
      "Epoch [1/5], Step [94/124], Loss: 0.3910\n",
      "Epoch [1/5], Step [95/124], Loss: 0.4620\n",
      "Epoch [1/5], Step [96/124], Loss: 0.4040\n",
      "Epoch [1/5], Step [97/124], Loss: 0.4065\n",
      "Epoch [1/5], Step [98/124], Loss: 0.3981\n",
      "Epoch [1/5], Step [99/124], Loss: 0.3626\n",
      "Epoch [1/5], Step [100/124], Loss: 0.3663\n",
      "Epoch [1/5], Step [101/124], Loss: 0.4236\n",
      "Epoch [1/5], Step [102/124], Loss: 0.4148\n",
      "Epoch [1/5], Step [103/124], Loss: 0.4133\n",
      "Epoch [1/5], Step [104/124], Loss: 0.3396\n",
      "Epoch [1/5], Step [105/124], Loss: 0.4305\n",
      "Epoch [1/5], Step [106/124], Loss: 0.3203\n",
      "Epoch [1/5], Step [107/124], Loss: 0.4222\n",
      "Epoch [1/5], Step [108/124], Loss: 0.4283\n",
      "Epoch [1/5], Step [109/124], Loss: 0.3847\n",
      "Epoch [1/5], Step [110/124], Loss: 0.3383\n",
      "Epoch [1/5], Step [111/124], Loss: 0.4198\n",
      "Epoch [1/5], Step [112/124], Loss: 0.4018\n",
      "Epoch [1/5], Step [113/124], Loss: 0.3841\n",
      "Epoch [1/5], Step [114/124], Loss: 0.4008\n",
      "Epoch [1/5], Step [115/124], Loss: 0.3929\n",
      "Epoch [1/5], Step [116/124], Loss: 0.4483\n",
      "Epoch [1/5], Step [117/124], Loss: 0.4191\n",
      "Epoch [1/5], Step [118/124], Loss: 0.3511\n",
      "Epoch [1/5], Step [119/124], Loss: 0.3826\n",
      "Epoch [1/5], Step [120/124], Loss: 0.2984\n",
      "Epoch [1/5], Step [121/124], Loss: 0.3167\n",
      "Epoch [1/5], Step [122/124], Loss: 0.3984\n",
      "Epoch [1/5], Step [123/124], Loss: 0.5112\n",
      "Epoch [1/5], Step [124/124], Loss: 0.4193\n",
      "Epoch [2/5], Step [1/124], Loss: 0.2648\n",
      "Epoch [2/5], Step [2/124], Loss: 0.4367\n",
      "Epoch [2/5], Step [3/124], Loss: 0.3914\n",
      "Epoch [2/5], Step [4/124], Loss: 0.4012\n",
      "Epoch [2/5], Step [5/124], Loss: 0.3065\n",
      "Epoch [2/5], Step [6/124], Loss: 0.3934\n",
      "Epoch [2/5], Step [7/124], Loss: 0.3263\n",
      "Epoch [2/5], Step [8/124], Loss: 0.4140\n",
      "Epoch [2/5], Step [9/124], Loss: 0.2874\n",
      "Epoch [2/5], Step [10/124], Loss: 0.4557\n",
      "Epoch [2/5], Step [11/124], Loss: 0.3601\n",
      "Epoch [2/5], Step [12/124], Loss: 0.3599\n",
      "Epoch [2/5], Step [13/124], Loss: 0.3302\n",
      "Epoch [2/5], Step [14/124], Loss: 0.3646\n",
      "Epoch [2/5], Step [15/124], Loss: 0.3973\n",
      "Epoch [2/5], Step [16/124], Loss: 0.3593\n",
      "Epoch [2/5], Step [17/124], Loss: 0.3419\n",
      "Epoch [2/5], Step [18/124], Loss: 0.2797\n",
      "Epoch [2/5], Step [19/124], Loss: 0.3081\n",
      "Epoch [2/5], Step [20/124], Loss: 0.3087\n",
      "Epoch [2/5], Step [21/124], Loss: 0.2275\n",
      "Epoch [2/5], Step [22/124], Loss: 0.2718\n",
      "Epoch [2/5], Step [23/124], Loss: 0.3799\n",
      "Epoch [2/5], Step [24/124], Loss: 0.3466\n",
      "Epoch [2/5], Step [25/124], Loss: 0.3360\n",
      "Epoch [2/5], Step [26/124], Loss: 0.2475\n",
      "Epoch [2/5], Step [27/124], Loss: 0.3245\n",
      "Epoch [2/5], Step [28/124], Loss: 0.3212\n",
      "Epoch [2/5], Step [29/124], Loss: 0.3082\n",
      "Epoch [2/5], Step [30/124], Loss: 0.2853\n",
      "Epoch [2/5], Step [31/124], Loss: 0.2890\n",
      "Epoch [2/5], Step [32/124], Loss: 0.2457\n",
      "Epoch [2/5], Step [33/124], Loss: 0.2036\n",
      "Epoch [2/5], Step [34/124], Loss: 0.3407\n",
      "Epoch [2/5], Step [35/124], Loss: 0.2780\n",
      "Epoch [2/5], Step [36/124], Loss: 0.2686\n",
      "Epoch [2/5], Step [37/124], Loss: 0.3022\n",
      "Epoch [2/5], Step [38/124], Loss: 0.3204\n",
      "Epoch [2/5], Step [39/124], Loss: 0.3318\n",
      "Epoch [2/5], Step [40/124], Loss: 0.2682\n",
      "Epoch [2/5], Step [41/124], Loss: 0.2612\n",
      "Epoch [2/5], Step [42/124], Loss: 0.2595\n",
      "Epoch [2/5], Step [43/124], Loss: 0.2907\n",
      "Epoch [2/5], Step [44/124], Loss: 0.2670\n",
      "Epoch [2/5], Step [45/124], Loss: 0.2531\n",
      "Epoch [2/5], Step [46/124], Loss: 0.2079\n",
      "Epoch [2/5], Step [47/124], Loss: 0.2562\n",
      "Epoch [2/5], Step [48/124], Loss: 0.3067\n",
      "Epoch [2/5], Step [49/124], Loss: 0.3356\n",
      "Epoch [2/5], Step [50/124], Loss: 0.3027\n",
      "Epoch [2/5], Step [51/124], Loss: 0.2781\n",
      "Epoch [2/5], Step [52/124], Loss: 0.2710\n",
      "Epoch [2/5], Step [53/124], Loss: 0.2977\n",
      "Epoch [2/5], Step [54/124], Loss: 0.2699\n",
      "Epoch [2/5], Step [55/124], Loss: 0.1960\n",
      "Epoch [2/5], Step [56/124], Loss: 0.3133\n",
      "Epoch [2/5], Step [57/124], Loss: 0.2550\n",
      "Epoch [2/5], Step [58/124], Loss: 0.2924\n",
      "Epoch [2/5], Step [59/124], Loss: 0.3418\n",
      "Epoch [2/5], Step [60/124], Loss: 0.2831\n",
      "Epoch [2/5], Step [61/124], Loss: 0.3108\n",
      "Epoch [2/5], Step [62/124], Loss: 0.3344\n",
      "Epoch [2/5], Step [63/124], Loss: 0.2620\n",
      "Epoch [2/5], Step [64/124], Loss: 0.2061\n",
      "Epoch [2/5], Step [65/124], Loss: 0.3248\n",
      "Epoch [2/5], Step [66/124], Loss: 0.3205\n",
      "Epoch [2/5], Step [67/124], Loss: 0.2722\n",
      "Epoch [2/5], Step [68/124], Loss: 0.2868\n",
      "Epoch [2/5], Step [69/124], Loss: 0.2735\n",
      "Epoch [2/5], Step [70/124], Loss: 0.2359\n",
      "Epoch [2/5], Step [71/124], Loss: 0.2388\n",
      "Epoch [2/5], Step [72/124], Loss: 0.3227\n",
      "Epoch [2/5], Step [73/124], Loss: 0.3608\n",
      "Epoch [2/5], Step [74/124], Loss: 0.1953\n",
      "Epoch [2/5], Step [75/124], Loss: 0.2184\n",
      "Epoch [2/5], Step [76/124], Loss: 0.3247\n",
      "Epoch [2/5], Step [77/124], Loss: 0.2437\n",
      "Epoch [2/5], Step [78/124], Loss: 0.3139\n",
      "Epoch [2/5], Step [79/124], Loss: 0.2188\n",
      "Epoch [2/5], Step [80/124], Loss: 0.1847\n",
      "Epoch [2/5], Step [81/124], Loss: 0.2476\n",
      "Epoch [2/5], Step [82/124], Loss: 0.3131\n",
      "Epoch [2/5], Step [83/124], Loss: 0.2974\n",
      "Epoch [2/5], Step [84/124], Loss: 0.2755\n",
      "Epoch [2/5], Step [85/124], Loss: 0.2658\n",
      "Epoch [2/5], Step [86/124], Loss: 0.2547\n",
      "Epoch [2/5], Step [87/124], Loss: 0.2970\n",
      "Epoch [2/5], Step [88/124], Loss: 0.2054\n",
      "Epoch [2/5], Step [89/124], Loss: 0.2204\n",
      "Epoch [2/5], Step [90/124], Loss: 0.2712\n",
      "Epoch [2/5], Step [91/124], Loss: 0.2052\n",
      "Epoch [2/5], Step [92/124], Loss: 0.1991\n",
      "Epoch [2/5], Step [93/124], Loss: 0.3598\n",
      "Epoch [2/5], Step [94/124], Loss: 0.3133\n",
      "Epoch [2/5], Step [95/124], Loss: 0.2747\n",
      "Epoch [2/5], Step [96/124], Loss: 0.2243\n",
      "Epoch [2/5], Step [97/124], Loss: 0.2215\n",
      "Epoch [2/5], Step [98/124], Loss: 0.3132\n",
      "Epoch [2/5], Step [99/124], Loss: 0.3108\n",
      "Epoch [2/5], Step [100/124], Loss: 0.3803\n",
      "Epoch [2/5], Step [101/124], Loss: 0.1858\n",
      "Epoch [2/5], Step [102/124], Loss: 0.2171\n",
      "Epoch [2/5], Step [103/124], Loss: 0.1943\n",
      "Epoch [2/5], Step [104/124], Loss: 0.2973\n",
      "Epoch [2/5], Step [105/124], Loss: 0.2117\n",
      "Epoch [2/5], Step [106/124], Loss: 0.1712\n",
      "Epoch [2/5], Step [107/124], Loss: 0.2423\n",
      "Epoch [2/5], Step [108/124], Loss: 0.2881\n",
      "Epoch [2/5], Step [109/124], Loss: 0.1928\n",
      "Epoch [2/5], Step [110/124], Loss: 0.2362\n",
      "Epoch [2/5], Step [111/124], Loss: 0.2354\n",
      "Epoch [2/5], Step [112/124], Loss: 0.3120\n",
      "Epoch [2/5], Step [113/124], Loss: 0.1958\n",
      "Epoch [2/5], Step [114/124], Loss: 0.2075\n",
      "Epoch [2/5], Step [115/124], Loss: 0.2434\n",
      "Epoch [2/5], Step [116/124], Loss: 0.2278\n",
      "Epoch [2/5], Step [117/124], Loss: 0.2500\n",
      "Epoch [2/5], Step [118/124], Loss: 0.2073\n",
      "Epoch [2/5], Step [119/124], Loss: 0.1994\n",
      "Epoch [2/5], Step [120/124], Loss: 0.1641\n",
      "Epoch [2/5], Step [121/124], Loss: 0.1364\n",
      "Epoch [2/5], Step [122/124], Loss: 0.2129\n",
      "Epoch [2/5], Step [123/124], Loss: 0.1563\n",
      "Epoch [2/5], Step [124/124], Loss: 0.3818\n",
      "Epoch [3/5], Step [1/124], Loss: 0.2503\n",
      "Epoch [3/5], Step [2/124], Loss: 0.2790\n",
      "Epoch [3/5], Step [3/124], Loss: 0.2431\n",
      "Epoch [3/5], Step [4/124], Loss: 0.1853\n",
      "Epoch [3/5], Step [5/124], Loss: 0.2012\n",
      "Epoch [3/5], Step [6/124], Loss: 0.1831\n",
      "Epoch [3/5], Step [7/124], Loss: 0.1782\n",
      "Epoch [3/5], Step [8/124], Loss: 0.2356\n",
      "Epoch [3/5], Step [9/124], Loss: 0.1885\n",
      "Epoch [3/5], Step [10/124], Loss: 0.2177\n",
      "Epoch [3/5], Step [11/124], Loss: 0.2273\n",
      "Epoch [3/5], Step [12/124], Loss: 0.3145\n",
      "Epoch [3/5], Step [13/124], Loss: 0.1961\n",
      "Epoch [3/5], Step [14/124], Loss: 0.2487\n",
      "Epoch [3/5], Step [15/124], Loss: 0.2766\n",
      "Epoch [3/5], Step [16/124], Loss: 0.1927\n",
      "Epoch [3/5], Step [17/124], Loss: 0.2155\n",
      "Epoch [3/5], Step [18/124], Loss: 0.2663\n",
      "Epoch [3/5], Step [19/124], Loss: 0.2202\n",
      "Epoch [3/5], Step [20/124], Loss: 0.2055\n",
      "Epoch [3/5], Step [21/124], Loss: 0.1637\n",
      "Epoch [3/5], Step [22/124], Loss: 0.2657\n",
      "Epoch [3/5], Step [23/124], Loss: 0.1185\n",
      "Epoch [3/5], Step [24/124], Loss: 0.1379\n",
      "Epoch [3/5], Step [25/124], Loss: 0.2731\n",
      "Epoch [3/5], Step [26/124], Loss: 0.1593\n",
      "Epoch [3/5], Step [27/124], Loss: 0.1914\n",
      "Epoch [3/5], Step [28/124], Loss: 0.1525\n",
      "Epoch [3/5], Step [29/124], Loss: 0.1857\n",
      "Epoch [3/5], Step [30/124], Loss: 0.1536\n",
      "Epoch [3/5], Step [31/124], Loss: 0.1527\n",
      "Epoch [3/5], Step [32/124], Loss: 0.1427\n",
      "Epoch [3/5], Step [33/124], Loss: 0.2233\n",
      "Epoch [3/5], Step [34/124], Loss: 0.1974\n",
      "Epoch [3/5], Step [35/124], Loss: 0.1877\n",
      "Epoch [3/5], Step [36/124], Loss: 0.1672\n",
      "Epoch [3/5], Step [37/124], Loss: 0.1502\n",
      "Epoch [3/5], Step [38/124], Loss: 0.1620\n",
      "Epoch [3/5], Step [39/124], Loss: 0.2065\n",
      "Epoch [3/5], Step [40/124], Loss: 0.2070\n",
      "Epoch [3/5], Step [41/124], Loss: 0.2287\n",
      "Epoch [3/5], Step [42/124], Loss: 0.1879\n",
      "Epoch [3/5], Step [43/124], Loss: 0.2794\n",
      "Epoch [3/5], Step [44/124], Loss: 0.1754\n",
      "Epoch [3/5], Step [45/124], Loss: 0.2374\n",
      "Epoch [3/5], Step [46/124], Loss: 0.1456\n",
      "Epoch [3/5], Step [47/124], Loss: 0.2042\n",
      "Epoch [3/5], Step [48/124], Loss: 0.1442\n",
      "Epoch [3/5], Step [49/124], Loss: 0.2030\n",
      "Epoch [3/5], Step [50/124], Loss: 0.1862\n",
      "Epoch [3/5], Step [51/124], Loss: 0.2570\n",
      "Epoch [3/5], Step [52/124], Loss: 0.1722\n",
      "Epoch [3/5], Step [53/124], Loss: 0.2471\n",
      "Epoch [3/5], Step [54/124], Loss: 0.2399\n",
      "Epoch [3/5], Step [55/124], Loss: 0.1960\n",
      "Epoch [3/5], Step [56/124], Loss: 0.2067\n",
      "Epoch [3/5], Step [57/124], Loss: 0.1464\n",
      "Epoch [3/5], Step [58/124], Loss: 0.1099\n",
      "Epoch [3/5], Step [59/124], Loss: 0.2345\n",
      "Epoch [3/5], Step [60/124], Loss: 0.1753\n",
      "Epoch [3/5], Step [61/124], Loss: 0.1785\n",
      "Epoch [3/5], Step [62/124], Loss: 0.1915\n",
      "Epoch [3/5], Step [63/124], Loss: 0.1375\n",
      "Epoch [3/5], Step [64/124], Loss: 0.1096\n",
      "Epoch [3/5], Step [65/124], Loss: 0.1585\n",
      "Epoch [3/5], Step [66/124], Loss: 0.2051\n",
      "Epoch [3/5], Step [67/124], Loss: 0.1787\n",
      "Epoch [3/5], Step [68/124], Loss: 0.1903\n",
      "Epoch [3/5], Step [69/124], Loss: 0.1650\n",
      "Epoch [3/5], Step [70/124], Loss: 0.1640\n",
      "Epoch [3/5], Step [71/124], Loss: 0.2154\n",
      "Epoch [3/5], Step [72/124], Loss: 0.1983\n",
      "Epoch [3/5], Step [73/124], Loss: 0.1259\n",
      "Epoch [3/5], Step [74/124], Loss: 0.1430\n",
      "Epoch [3/5], Step [75/124], Loss: 0.1633\n",
      "Epoch [3/5], Step [76/124], Loss: 0.2523\n",
      "Epoch [3/5], Step [77/124], Loss: 0.1650\n",
      "Epoch [3/5], Step [78/124], Loss: 0.1563\n",
      "Epoch [3/5], Step [79/124], Loss: 0.2010\n",
      "Epoch [3/5], Step [80/124], Loss: 0.1665\n",
      "Epoch [3/5], Step [81/124], Loss: 0.1400\n",
      "Epoch [3/5], Step [82/124], Loss: 0.1770\n",
      "Epoch [3/5], Step [83/124], Loss: 0.1387\n",
      "Epoch [3/5], Step [84/124], Loss: 0.1267\n",
      "Epoch [3/5], Step [85/124], Loss: 0.2130\n",
      "Epoch [3/5], Step [86/124], Loss: 0.1473\n",
      "Epoch [3/5], Step [87/124], Loss: 0.1906\n",
      "Epoch [3/5], Step [88/124], Loss: 0.1880\n",
      "Epoch [3/5], Step [89/124], Loss: 0.2282\n",
      "Epoch [3/5], Step [90/124], Loss: 0.1527\n",
      "Epoch [3/5], Step [91/124], Loss: 0.1790\n",
      "Epoch [3/5], Step [92/124], Loss: 0.2391\n",
      "Epoch [3/5], Step [93/124], Loss: 0.1959\n",
      "Epoch [3/5], Step [94/124], Loss: 0.1902\n",
      "Epoch [3/5], Step [95/124], Loss: 0.1152\n",
      "Epoch [3/5], Step [96/124], Loss: 0.1706\n",
      "Epoch [3/5], Step [97/124], Loss: 0.1778\n",
      "Epoch [3/5], Step [98/124], Loss: 0.2057\n",
      "Epoch [3/5], Step [99/124], Loss: 0.1417\n",
      "Epoch [3/5], Step [100/124], Loss: 0.1833\n",
      "Epoch [3/5], Step [101/124], Loss: 0.1670\n",
      "Epoch [3/5], Step [102/124], Loss: 0.2024\n",
      "Epoch [3/5], Step [103/124], Loss: 0.1342\n",
      "Epoch [3/5], Step [104/124], Loss: 0.1537\n",
      "Epoch [3/5], Step [105/124], Loss: 0.0941\n",
      "Epoch [3/5], Step [106/124], Loss: 0.1305\n",
      "Epoch [3/5], Step [107/124], Loss: 0.1970\n",
      "Epoch [3/5], Step [108/124], Loss: 0.1201\n",
      "Epoch [3/5], Step [109/124], Loss: 0.1648\n",
      "Epoch [3/5], Step [110/124], Loss: 0.1947\n",
      "Epoch [3/5], Step [111/124], Loss: 0.1198\n",
      "Epoch [3/5], Step [112/124], Loss: 0.2311\n",
      "Epoch [3/5], Step [113/124], Loss: 0.1625\n",
      "Epoch [3/5], Step [114/124], Loss: 0.1797\n",
      "Epoch [3/5], Step [115/124], Loss: 0.1094\n",
      "Epoch [3/5], Step [116/124], Loss: 0.1852\n",
      "Epoch [3/5], Step [117/124], Loss: 0.1938\n",
      "Epoch [3/5], Step [118/124], Loss: 0.0964\n",
      "Epoch [3/5], Step [119/124], Loss: 0.1553\n",
      "Epoch [3/5], Step [120/124], Loss: 0.1962\n",
      "Epoch [3/5], Step [121/124], Loss: 0.2669\n",
      "Epoch [3/5], Step [122/124], Loss: 0.1782\n",
      "Epoch [3/5], Step [123/124], Loss: 0.2032\n",
      "Epoch [3/5], Step [124/124], Loss: 0.3379\n",
      "Epoch [4/5], Step [1/124], Loss: 0.1534\n",
      "Epoch [4/5], Step [2/124], Loss: 0.1843\n",
      "Epoch [4/5], Step [3/124], Loss: 0.2299\n",
      "Epoch [4/5], Step [4/124], Loss: 0.1981\n",
      "Epoch [4/5], Step [5/124], Loss: 0.1685\n",
      "Epoch [4/5], Step [6/124], Loss: 0.1570\n",
      "Epoch [4/5], Step [7/124], Loss: 0.2214\n",
      "Epoch [4/5], Step [8/124], Loss: 0.1814\n",
      "Epoch [4/5], Step [9/124], Loss: 0.1770\n",
      "Epoch [4/5], Step [10/124], Loss: 0.2594\n",
      "Epoch [4/5], Step [11/124], Loss: 0.1503\n",
      "Epoch [4/5], Step [12/124], Loss: 0.1708\n",
      "Epoch [4/5], Step [13/124], Loss: 0.1394\n",
      "Epoch [4/5], Step [14/124], Loss: 0.1303\n",
      "Epoch [4/5], Step [15/124], Loss: 0.1027\n",
      "Epoch [4/5], Step [16/124], Loss: 0.2538\n",
      "Epoch [4/5], Step [17/124], Loss: 0.1482\n",
      "Epoch [4/5], Step [18/124], Loss: 0.1501\n",
      "Epoch [4/5], Step [19/124], Loss: 0.0985\n",
      "Epoch [4/5], Step [20/124], Loss: 0.1483\n",
      "Epoch [4/5], Step [21/124], Loss: 0.1436\n",
      "Epoch [4/5], Step [22/124], Loss: 0.1335\n",
      "Epoch [4/5], Step [23/124], Loss: 0.1508\n",
      "Epoch [4/5], Step [24/124], Loss: 0.1342\n",
      "Epoch [4/5], Step [25/124], Loss: 0.1751\n",
      "Epoch [4/5], Step [26/124], Loss: 0.1304\n",
      "Epoch [4/5], Step [27/124], Loss: 0.2033\n",
      "Epoch [4/5], Step [28/124], Loss: 0.1684\n",
      "Epoch [4/5], Step [29/124], Loss: 0.1812\n",
      "Epoch [4/5], Step [30/124], Loss: 0.1919\n",
      "Epoch [4/5], Step [31/124], Loss: 0.1446\n",
      "Epoch [4/5], Step [32/124], Loss: 0.1954\n",
      "Epoch [4/5], Step [33/124], Loss: 0.1262\n",
      "Epoch [4/5], Step [34/124], Loss: 0.1076\n",
      "Epoch [4/5], Step [35/124], Loss: 0.1116\n",
      "Epoch [4/5], Step [36/124], Loss: 0.1368\n",
      "Epoch [4/5], Step [37/124], Loss: 0.0968\n",
      "Epoch [4/5], Step [38/124], Loss: 0.0909\n",
      "Epoch [4/5], Step [39/124], Loss: 0.1847\n",
      "Epoch [4/5], Step [40/124], Loss: 0.1738\n",
      "Epoch [4/5], Step [41/124], Loss: 0.1086\n",
      "Epoch [4/5], Step [42/124], Loss: 0.1221\n",
      "Epoch [4/5], Step [43/124], Loss: 0.1530\n",
      "Epoch [4/5], Step [44/124], Loss: 0.1848\n",
      "Epoch [4/5], Step [45/124], Loss: 0.1164\n",
      "Epoch [4/5], Step [46/124], Loss: 0.1167\n",
      "Epoch [4/5], Step [47/124], Loss: 0.1444\n",
      "Epoch [4/5], Step [48/124], Loss: 0.0932\n",
      "Epoch [4/5], Step [49/124], Loss: 0.1711\n",
      "Epoch [4/5], Step [50/124], Loss: 0.1339\n",
      "Epoch [4/5], Step [51/124], Loss: 0.1020\n",
      "Epoch [4/5], Step [52/124], Loss: 0.1874\n",
      "Epoch [4/5], Step [53/124], Loss: 0.1166\n",
      "Epoch [4/5], Step [54/124], Loss: 0.2201\n",
      "Epoch [4/5], Step [55/124], Loss: 0.1865\n",
      "Epoch [4/5], Step [56/124], Loss: 0.2158\n",
      "Epoch [4/5], Step [57/124], Loss: 0.1732\n",
      "Epoch [4/5], Step [58/124], Loss: 0.0874\n",
      "Epoch [4/5], Step [59/124], Loss: 0.1448\n",
      "Epoch [4/5], Step [60/124], Loss: 0.0938\n",
      "Epoch [4/5], Step [61/124], Loss: 0.1361\n",
      "Epoch [4/5], Step [62/124], Loss: 0.1727\n",
      "Epoch [4/5], Step [63/124], Loss: 0.1595\n",
      "Epoch [4/5], Step [64/124], Loss: 0.0940\n",
      "Epoch [4/5], Step [65/124], Loss: 0.1679\n",
      "Epoch [4/5], Step [66/124], Loss: 0.1135\n",
      "Epoch [4/5], Step [67/124], Loss: 0.1739\n",
      "Epoch [4/5], Step [68/124], Loss: 0.1421\n",
      "Epoch [4/5], Step [69/124], Loss: 0.0425\n",
      "Epoch [4/5], Step [70/124], Loss: 0.1114\n",
      "Epoch [4/5], Step [71/124], Loss: 0.1327\n",
      "Epoch [4/5], Step [72/124], Loss: 0.1451\n",
      "Epoch [4/5], Step [73/124], Loss: 0.1143\n",
      "Epoch [4/5], Step [74/124], Loss: 0.1543\n",
      "Epoch [4/5], Step [75/124], Loss: 0.1444\n",
      "Epoch [4/5], Step [76/124], Loss: 0.1105\n",
      "Epoch [4/5], Step [77/124], Loss: 0.1264\n",
      "Epoch [4/5], Step [78/124], Loss: 0.1342\n",
      "Epoch [4/5], Step [79/124], Loss: 0.1411\n",
      "Epoch [4/5], Step [80/124], Loss: 0.1212\n",
      "Epoch [4/5], Step [81/124], Loss: 0.1760\n",
      "Epoch [4/5], Step [82/124], Loss: 0.0901\n",
      "Epoch [4/5], Step [83/124], Loss: 0.1152\n",
      "Epoch [4/5], Step [84/124], Loss: 0.0753\n",
      "Epoch [4/5], Step [85/124], Loss: 0.1519\n",
      "Epoch [4/5], Step [86/124], Loss: 0.0999\n",
      "Epoch [4/5], Step [87/124], Loss: 0.0568\n",
      "Epoch [4/5], Step [88/124], Loss: 0.1309\n",
      "Epoch [4/5], Step [89/124], Loss: 0.1090\n",
      "Epoch [4/5], Step [90/124], Loss: 0.0971\n",
      "Epoch [4/5], Step [91/124], Loss: 0.1010\n",
      "Epoch [4/5], Step [92/124], Loss: 0.0890\n",
      "Epoch [4/5], Step [93/124], Loss: 0.0910\n",
      "Epoch [4/5], Step [94/124], Loss: 0.1179\n",
      "Epoch [4/5], Step [95/124], Loss: 0.1122\n",
      "Epoch [4/5], Step [96/124], Loss: 0.1289\n",
      "Epoch [4/5], Step [97/124], Loss: 0.1459\n",
      "Epoch [4/5], Step [98/124], Loss: 0.0817\n",
      "Epoch [4/5], Step [99/124], Loss: 0.1706\n",
      "Epoch [4/5], Step [100/124], Loss: 0.1009\n",
      "Epoch [4/5], Step [101/124], Loss: 0.1413\n",
      "Epoch [4/5], Step [102/124], Loss: 0.1568\n",
      "Epoch [4/5], Step [103/124], Loss: 0.0818\n",
      "Epoch [4/5], Step [104/124], Loss: 0.1460\n",
      "Epoch [4/5], Step [105/124], Loss: 0.0724\n",
      "Epoch [4/5], Step [106/124], Loss: 0.1264\n",
      "Epoch [4/5], Step [107/124], Loss: 0.1338\n",
      "Epoch [4/5], Step [108/124], Loss: 0.1572\n",
      "Epoch [4/5], Step [109/124], Loss: 0.1358\n",
      "Epoch [4/5], Step [110/124], Loss: 0.1133\n",
      "Epoch [4/5], Step [111/124], Loss: 0.1371\n",
      "Epoch [4/5], Step [112/124], Loss: 0.0797\n",
      "Epoch [4/5], Step [113/124], Loss: 0.0967\n",
      "Epoch [4/5], Step [114/124], Loss: 0.1222\n",
      "Epoch [4/5], Step [115/124], Loss: 0.0969\n",
      "Epoch [4/5], Step [116/124], Loss: 0.1112\n",
      "Epoch [4/5], Step [117/124], Loss: 0.1406\n",
      "Epoch [4/5], Step [118/124], Loss: 0.1265\n",
      "Epoch [4/5], Step [119/124], Loss: 0.1082\n",
      "Epoch [4/5], Step [120/124], Loss: 0.1225\n",
      "Epoch [4/5], Step [121/124], Loss: 0.1672\n",
      "Epoch [4/5], Step [122/124], Loss: 0.1068\n",
      "Epoch [4/5], Step [123/124], Loss: 0.1122\n",
      "Epoch [4/5], Step [124/124], Loss: 0.7917\n",
      "Epoch [5/5], Step [1/124], Loss: 0.1049\n",
      "Epoch [5/5], Step [2/124], Loss: 0.2784\n",
      "Epoch [5/5], Step [3/124], Loss: 0.2414\n",
      "Epoch [5/5], Step [4/124], Loss: 0.1699\n",
      "Epoch [5/5], Step [5/124], Loss: 0.2037\n",
      "Epoch [5/5], Step [6/124], Loss: 0.0900\n",
      "Epoch [5/5], Step [7/124], Loss: 0.1401\n",
      "Epoch [5/5], Step [8/124], Loss: 0.1429\n",
      "Epoch [5/5], Step [9/124], Loss: 0.1665\n",
      "Epoch [5/5], Step [10/124], Loss: 0.1591\n",
      "Epoch [5/5], Step [11/124], Loss: 0.1350\n",
      "Epoch [5/5], Step [12/124], Loss: 0.2252\n",
      "Epoch [5/5], Step [13/124], Loss: 0.1554\n",
      "Epoch [5/5], Step [14/124], Loss: 0.1266\n",
      "Epoch [5/5], Step [15/124], Loss: 0.0470\n",
      "Epoch [5/5], Step [16/124], Loss: 0.1178\n",
      "Epoch [5/5], Step [17/124], Loss: 0.1703\n",
      "Epoch [5/5], Step [18/124], Loss: 0.2011\n",
      "Epoch [5/5], Step [19/124], Loss: 0.0662\n",
      "Epoch [5/5], Step [20/124], Loss: 0.1719\n",
      "Epoch [5/5], Step [21/124], Loss: 0.0962\n",
      "Epoch [5/5], Step [22/124], Loss: 0.1196\n",
      "Epoch [5/5], Step [23/124], Loss: 0.1058\n",
      "Epoch [5/5], Step [24/124], Loss: 0.0940\n",
      "Epoch [5/5], Step [25/124], Loss: 0.1190\n",
      "Epoch [5/5], Step [26/124], Loss: 0.1130\n",
      "Epoch [5/5], Step [27/124], Loss: 0.1523\n",
      "Epoch [5/5], Step [28/124], Loss: 0.1209\n",
      "Epoch [5/5], Step [29/124], Loss: 0.1340\n",
      "Epoch [5/5], Step [30/124], Loss: 0.0873\n",
      "Epoch [5/5], Step [31/124], Loss: 0.1350\n",
      "Epoch [5/5], Step [32/124], Loss: 0.1309\n",
      "Epoch [5/5], Step [33/124], Loss: 0.1079\n",
      "Epoch [5/5], Step [34/124], Loss: 0.1005\n",
      "Epoch [5/5], Step [35/124], Loss: 0.1168\n",
      "Epoch [5/5], Step [36/124], Loss: 0.1433\n",
      "Epoch [5/5], Step [37/124], Loss: 0.1073\n",
      "Epoch [5/5], Step [38/124], Loss: 0.1370\n",
      "Epoch [5/5], Step [39/124], Loss: 0.1238\n",
      "Epoch [5/5], Step [40/124], Loss: 0.1367\n",
      "Epoch [5/5], Step [41/124], Loss: 0.0755\n",
      "Epoch [5/5], Step [42/124], Loss: 0.0982\n",
      "Epoch [5/5], Step [43/124], Loss: 0.1187\n",
      "Epoch [5/5], Step [44/124], Loss: 0.1441\n",
      "Epoch [5/5], Step [45/124], Loss: 0.1399\n",
      "Epoch [5/5], Step [46/124], Loss: 0.1133\n",
      "Epoch [5/5], Step [47/124], Loss: 0.0773\n",
      "Epoch [5/5], Step [48/124], Loss: 0.1197\n",
      "Epoch [5/5], Step [49/124], Loss: 0.1700\n",
      "Epoch [5/5], Step [50/124], Loss: 0.1371\n",
      "Epoch [5/5], Step [51/124], Loss: 0.1022\n",
      "Epoch [5/5], Step [52/124], Loss: 0.1067\n",
      "Epoch [5/5], Step [53/124], Loss: 0.1131\n",
      "Epoch [5/5], Step [54/124], Loss: 0.0702\n",
      "Epoch [5/5], Step [55/124], Loss: 0.0901\n",
      "Epoch [5/5], Step [56/124], Loss: 0.0816\n",
      "Epoch [5/5], Step [57/124], Loss: 0.0677\n",
      "Epoch [5/5], Step [58/124], Loss: 0.0692\n",
      "Epoch [5/5], Step [59/124], Loss: 0.0613\n",
      "Epoch [5/5], Step [60/124], Loss: 0.0528\n",
      "Epoch [5/5], Step [61/124], Loss: 0.1538\n",
      "Epoch [5/5], Step [62/124], Loss: 0.0699\n",
      "Epoch [5/5], Step [63/124], Loss: 0.1090\n",
      "Epoch [5/5], Step [64/124], Loss: 0.1237\n",
      "Epoch [5/5], Step [65/124], Loss: 0.0990\n",
      "Epoch [5/5], Step [66/124], Loss: 0.1182\n",
      "Epoch [5/5], Step [67/124], Loss: 0.0914\n",
      "Epoch [5/5], Step [68/124], Loss: 0.0599\n",
      "Epoch [5/5], Step [69/124], Loss: 0.0853\n",
      "Epoch [5/5], Step [70/124], Loss: 0.0713\n",
      "Epoch [5/5], Step [71/124], Loss: 0.1269\n",
      "Epoch [5/5], Step [72/124], Loss: 0.1759\n",
      "Epoch [5/5], Step [73/124], Loss: 0.1018\n",
      "Epoch [5/5], Step [74/124], Loss: 0.1082\n",
      "Epoch [5/5], Step [75/124], Loss: 0.0980\n",
      "Epoch [5/5], Step [76/124], Loss: 0.0575\n",
      "Epoch [5/5], Step [77/124], Loss: 0.1223\n",
      "Epoch [5/5], Step [78/124], Loss: 0.1036\n",
      "Epoch [5/5], Step [79/124], Loss: 0.1029\n",
      "Epoch [5/5], Step [80/124], Loss: 0.0890\n",
      "Epoch [5/5], Step [81/124], Loss: 0.1140\n",
      "Epoch [5/5], Step [82/124], Loss: 0.1158\n",
      "Epoch [5/5], Step [83/124], Loss: 0.1114\n",
      "Epoch [5/5], Step [84/124], Loss: 0.1618\n",
      "Epoch [5/5], Step [85/124], Loss: 0.0922\n",
      "Epoch [5/5], Step [86/124], Loss: 0.0936\n",
      "Epoch [5/5], Step [87/124], Loss: 0.1128\n",
      "Epoch [5/5], Step [88/124], Loss: 0.0796\n",
      "Epoch [5/5], Step [89/124], Loss: 0.1313\n",
      "Epoch [5/5], Step [90/124], Loss: 0.0762\n",
      "Epoch [5/5], Step [91/124], Loss: 0.1453\n",
      "Epoch [5/5], Step [92/124], Loss: 0.1224\n",
      "Epoch [5/5], Step [93/124], Loss: 0.0902\n",
      "Epoch [5/5], Step [94/124], Loss: 0.0876\n",
      "Epoch [5/5], Step [95/124], Loss: 0.0651\n",
      "Epoch [5/5], Step [96/124], Loss: 0.0913\n",
      "Epoch [5/5], Step [97/124], Loss: 0.0848\n",
      "Epoch [5/5], Step [98/124], Loss: 0.1344\n",
      "Epoch [5/5], Step [99/124], Loss: 0.0922\n",
      "Epoch [5/5], Step [100/124], Loss: 0.0825\n",
      "Epoch [5/5], Step [101/124], Loss: 0.0979\n",
      "Epoch [5/5], Step [102/124], Loss: 0.1124\n",
      "Epoch [5/5], Step [103/124], Loss: 0.0795\n",
      "Epoch [5/5], Step [104/124], Loss: 0.1083\n",
      "Epoch [5/5], Step [105/124], Loss: 0.0808\n",
      "Epoch [5/5], Step [106/124], Loss: 0.0935\n",
      "Epoch [5/5], Step [107/124], Loss: 0.0920\n",
      "Epoch [5/5], Step [108/124], Loss: 0.0743\n",
      "Epoch [5/5], Step [109/124], Loss: 0.0957\n",
      "Epoch [5/5], Step [110/124], Loss: 0.1190\n",
      "Epoch [5/5], Step [111/124], Loss: 0.1575\n",
      "Epoch [5/5], Step [112/124], Loss: 0.1857\n",
      "Epoch [5/5], Step [113/124], Loss: 0.0890\n",
      "Epoch [5/5], Step [114/124], Loss: 0.1167\n",
      "Epoch [5/5], Step [115/124], Loss: 0.1102\n",
      "Epoch [5/5], Step [116/124], Loss: 0.1205\n",
      "Epoch [5/5], Step [117/124], Loss: 0.0765\n",
      "Epoch [5/5], Step [118/124], Loss: 0.1048\n",
      "Epoch [5/5], Step [119/124], Loss: 0.0937\n",
      "Epoch [5/5], Step [120/124], Loss: 0.0799\n",
      "Epoch [5/5], Step [121/124], Loss: 0.0887\n",
      "Epoch [5/5], Step [122/124], Loss: 0.0688\n",
      "Epoch [5/5], Step [123/124], Loss: 0.0772\n",
      "Epoch [5/5], Step [124/124], Loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        #print(images.shape)\n",
    "        #images = images.reshape(images.shape[0], -1)\n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "       \n",
    "        print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:28.933438Z",
     "iopub.status.busy": "2021-12-10T03:50:28.933201Z",
     "iopub.status.idle": "2021-12-10T03:50:28.940632Z",
     "shell.execute_reply": "2021-12-10T03:50:28.939536Z",
     "shell.execute_reply.started": "2021-12-10T03:50:28.933409Z"
    },
    "id": "zRMsuxvqXPc7"
   },
   "outputs": [],
   "source": [
    "def check_acc (loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            outputs = model(x)\n",
    "            _, pred = outputs.max(1)\n",
    "            num_correct += (pred==y).sum()\n",
    "            num_samples += pred.size(0)\n",
    "            print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct/float(num_samples)*100):.2f}')\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:28.942125Z",
     "iopub.status.busy": "2021-12-10T03:50:28.941884Z",
     "iopub.status.idle": "2021-12-10T03:50:30.655117Z",
     "shell.execute_reply": "2021-12-10T03:50:30.654161Z",
     "shell.execute_reply.started": "2021-12-10T03:50:28.942095Z"
    },
    "id": "ai_2_LTIcpza",
    "outputId": "217fd1e6-ef30-4ad7-93d0-07b5d79a25a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 249/256 with accuracy 97.27\n",
      "Got 498/512 with accuracy 97.27\n",
      "Got 748/768 with accuracy 97.40\n",
      "Got 998/1024 with accuracy 97.46\n",
      "Got 1240/1280 with accuracy 96.88\n",
      "Got 1486/1536 with accuracy 96.74\n",
      "Got 1730/1792 with accuracy 96.54\n",
      "Got 1978/2048 with accuracy 96.58\n",
      "Got 2221/2304 with accuracy 96.40\n",
      "Got 2471/2560 with accuracy 96.52\n",
      "Got 2724/2816 with accuracy 96.73\n",
      "Got 2972/3072 with accuracy 96.74\n",
      "Got 3221/3328 with accuracy 96.78\n",
      "Got 3469/3584 with accuracy 96.79\n",
      "Got 3721/3840 with accuracy 96.90\n",
      "Got 3972/4096 with accuracy 96.97\n",
      "Got 4226/4352 with accuracy 97.10\n",
      "Got 4476/4608 with accuracy 97.14\n",
      "Got 4728/4864 with accuracy 97.20\n",
      "Got 4974/5120 with accuracy 97.15\n",
      "Got 5227/5376 with accuracy 97.23\n",
      "Got 5476/5632 with accuracy 97.23\n",
      "Got 5720/5888 with accuracy 97.15\n",
      "Got 5970/6144 with accuracy 97.17\n",
      "Got 6221/6400 with accuracy 97.20\n",
      "Got 6471/6656 with accuracy 97.22\n",
      "Got 6722/6912 with accuracy 97.25\n",
      "Got 6972/7168 with accuracy 97.27\n",
      "Got 7223/7424 with accuracy 97.29\n",
      "Got 7471/7680 with accuracy 97.28\n",
      "Got 7720/7936 with accuracy 97.28\n",
      "Got 7969/8192 with accuracy 97.28\n",
      "Got 8219/8448 with accuracy 97.29\n",
      "Got 8469/8704 with accuracy 97.30\n",
      "Got 8719/8960 with accuracy 97.31\n",
      "Got 8964/9216 with accuracy 97.27\n",
      "Got 9212/9472 with accuracy 97.26\n",
      "Got 9460/9728 with accuracy 97.25\n",
      "Got 9708/9984 with accuracy 97.24\n",
      "Got 9958/10240 with accuracy 97.25\n",
      "Got 10206/10496 with accuracy 97.24\n",
      "Got 10456/10752 with accuracy 97.25\n",
      "Got 10706/11008 with accuracy 97.26\n",
      "Got 10950/11264 with accuracy 97.21\n",
      "Got 11199/11520 with accuracy 97.21\n",
      "Got 11448/11776 with accuracy 97.21\n",
      "Got 11699/12032 with accuracy 97.23\n",
      "Got 11949/12288 with accuracy 97.24\n",
      "Got 12197/12544 with accuracy 97.23\n",
      "Got 12448/12800 with accuracy 97.25\n",
      "Got 12700/13056 with accuracy 97.27\n",
      "Got 12953/13312 with accuracy 97.30\n",
      "Got 13202/13568 with accuracy 97.30\n",
      "Got 13454/13824 with accuracy 97.32\n",
      "Got 13703/14080 with accuracy 97.32\n",
      "Got 13955/14336 with accuracy 97.34\n",
      "Got 14204/14592 with accuracy 97.34\n",
      "Got 14448/14848 with accuracy 97.31\n",
      "Got 14697/15104 with accuracy 97.31\n",
      "Got 14947/15360 with accuracy 97.31\n",
      "Got 15195/15616 with accuracy 97.30\n",
      "Got 15447/15872 with accuracy 97.32\n",
      "Got 15696/16128 with accuracy 97.32\n",
      "Got 15941/16384 with accuracy 97.30\n",
      "Got 16185/16640 with accuracy 97.27\n",
      "Got 16434/16896 with accuracy 97.27\n",
      "Got 16680/17152 with accuracy 97.25\n",
      "Got 16929/17408 with accuracy 97.25\n",
      "Got 17179/17664 with accuracy 97.25\n",
      "Got 17425/17920 with accuracy 97.24\n",
      "Got 17675/18176 with accuracy 97.24\n",
      "Got 17927/18432 with accuracy 97.26\n",
      "Got 18177/18688 with accuracy 97.27\n",
      "Got 18430/18944 with accuracy 97.29\n",
      "Got 18678/19200 with accuracy 97.28\n",
      "Got 18929/19456 with accuracy 97.29\n",
      "Got 19176/19712 with accuracy 97.28\n",
      "Got 19420/19968 with accuracy 97.26\n",
      "Got 19670/20224 with accuracy 97.26\n",
      "Got 19914/20480 with accuracy 97.24\n",
      "Got 20163/20736 with accuracy 97.24\n",
      "Got 20414/20992 with accuracy 97.25\n",
      "Got 20665/21248 with accuracy 97.26\n",
      "Got 20913/21504 with accuracy 97.25\n",
      "Got 21165/21760 with accuracy 97.27\n",
      "Got 21413/22016 with accuracy 97.26\n",
      "Got 21661/22272 with accuracy 97.26\n",
      "Got 21909/22528 with accuracy 97.25\n",
      "Got 22157/22784 with accuracy 97.25\n",
      "Got 22404/23040 with accuracy 97.24\n",
      "Got 22648/23296 with accuracy 97.22\n",
      "Got 22891/23552 with accuracy 97.19\n",
      "Got 23145/23808 with accuracy 97.22\n",
      "Got 23390/24064 with accuracy 97.20\n",
      "Got 23635/24320 with accuracy 97.18\n",
      "Got 23883/24576 with accuracy 97.18\n",
      "Got 24131/24832 with accuracy 97.18\n",
      "Got 24383/25088 with accuracy 97.19\n",
      "Got 24630/25344 with accuracy 97.18\n",
      "Got 24878/25600 with accuracy 97.18\n",
      "Got 25126/25856 with accuracy 97.18\n",
      "Got 25372/26112 with accuracy 97.17\n",
      "Got 25625/26368 with accuracy 97.18\n",
      "Got 25872/26624 with accuracy 97.18\n",
      "Got 26121/26880 with accuracy 97.18\n",
      "Got 26372/27136 with accuracy 97.18\n",
      "Got 26622/27392 with accuracy 97.19\n",
      "Got 26868/27648 with accuracy 97.18\n",
      "Got 27116/27904 with accuracy 97.18\n",
      "Got 27365/28160 with accuracy 97.18\n",
      "Got 27614/28416 with accuracy 97.18\n",
      "Got 27861/28672 with accuracy 97.17\n",
      "Got 28108/28928 with accuracy 97.17\n",
      "Got 28354/29184 with accuracy 97.16\n",
      "Got 28606/29440 with accuracy 97.17\n",
      "Got 28857/29696 with accuracy 97.17\n",
      "Got 29108/29952 with accuracy 97.18\n",
      "Got 29356/30208 with accuracy 97.18\n",
      "Got 29607/30464 with accuracy 97.19\n",
      "Got 29858/30720 with accuracy 97.19\n",
      "Got 30110/30976 with accuracy 97.20\n",
      "Got 30359/31232 with accuracy 97.20\n",
      "Got 30606/31488 with accuracy 97.20\n",
      "Got 30618/31500 with accuracy 97.20\n"
     ]
    }
   ],
   "source": [
    "check_acc(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:55:23.631934Z",
     "iopub.status.busy": "2021-12-10T03:55:23.631278Z",
     "iopub.status.idle": "2021-12-10T03:55:24.3441Z",
     "shell.execute_reply": "2021-12-10T03:55:24.343215Z",
     "shell.execute_reply.started": "2021-12-10T03:55:23.631891Z"
    },
    "id": "lwHWKv10ct9b",
    "outputId": "1eb4af3e-0120-4e56-db82-0a9bc35b22cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 240/256 with accuracy 93.75\n",
      "Got 486/512 with accuracy 94.92\n",
      "Got 726/768 with accuracy 94.53\n",
      "Got 973/1024 with accuracy 95.02\n",
      "Got 1223/1280 with accuracy 95.55\n",
      "Got 1472/1536 with accuracy 95.83\n",
      "Got 1720/1792 with accuracy 95.98\n",
      "Got 1969/2048 with accuracy 96.14\n",
      "Got 2221/2304 with accuracy 96.40\n",
      "Got 2468/2560 with accuracy 96.41\n",
      "Got 2719/2816 with accuracy 96.56\n",
      "Got 2968/3072 with accuracy 96.61\n",
      "Got 3216/3328 with accuracy 96.63\n",
      "Got 3463/3584 with accuracy 96.62\n",
      "Got 3713/3840 with accuracy 96.69\n",
      "Got 3961/4096 with accuracy 96.70\n",
      "Got 4213/4352 with accuracy 96.81\n",
      "Got 4464/4608 with accuracy 96.88\n",
      "Got 4717/4864 with accuracy 96.98\n",
      "Got 4965/5120 with accuracy 96.97\n",
      "Got 5212/5376 with accuracy 96.95\n",
      "Got 5463/5632 with accuracy 97.00\n",
      "Got 5714/5888 with accuracy 97.04\n",
      "Got 5962/6144 with accuracy 97.04\n",
      "Got 6209/6400 with accuracy 97.02\n",
      "Got 6461/6656 with accuracy 97.07\n",
      "Got 6710/6912 with accuracy 97.08\n",
      "Got 6957/7168 with accuracy 97.06\n",
      "Got 7204/7424 with accuracy 97.04\n",
      "Got 7453/7680 with accuracy 97.04\n",
      "Got 7697/7936 with accuracy 96.99\n",
      "Got 7945/8192 with accuracy 96.98\n",
      "Got 8195/8448 with accuracy 97.01\n",
      "Got 8443/8704 with accuracy 97.00\n",
      "Got 8688/8960 with accuracy 96.96\n",
      "Got 8937/9216 with accuracy 96.97\n",
      "Got 9182/9472 with accuracy 96.94\n",
      "Got 9426/9728 with accuracy 96.90\n",
      "Got 9674/9984 with accuracy 96.90\n",
      "Got 9919/10240 with accuracy 96.87\n",
      "Got 10170/10496 with accuracy 96.89\n",
      "Got 10174/10500 with accuracy 96.90\n"
     ]
    }
   ],
   "source": [
    "check_acc(val_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:03:14.153979Z",
     "iopub.status.busy": "2021-12-10T04:03:14.153697Z",
     "iopub.status.idle": "2021-12-10T04:03:14.158271Z",
     "shell.execute_reply": "2021-12-10T04:03:14.157603Z",
     "shell.execute_reply.started": "2021-12-10T04:03:14.153945Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader=DataLoader(x_test,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:04:49.108254Z",
     "iopub.status.busy": "2021-12-10T04:04:49.107963Z",
     "iopub.status.idle": "2021-12-10T04:05:01.329610Z",
     "shell.execute_reply": "2021-12-10T04:05:01.328717Z",
     "shell.execute_reply.started": "2021-12-10T04:04:49.108222Z"
    }
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame(columns=['ImageId','Label'])\n",
    "serial=[]\n",
    "ans=[]\n",
    "model.eval()\n",
    "for s,i in enumerate(test_loader):\n",
    "    serial.append(s+1)\n",
    "    ans.append(model(i).argmax())\n",
    "sub['ImageId']=serial    \n",
    "sub['Label']=np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:05:30.591419Z",
     "iopub.status.busy": "2021-12-10T04:05:30.590846Z",
     "iopub.status.idle": "2021-12-10T04:05:30.650194Z",
     "shell.execute_reply": "2021-12-10T04:05:30.649317Z",
     "shell.execute_reply.started": "2021-12-10T04:05:30.591367Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:05:38.315723Z",
     "iopub.status.busy": "2021-12-10T04:05:38.315418Z",
     "iopub.status.idle": "2021-12-10T04:05:38.333904Z",
     "shell.execute_reply": "2021-12-10T04:05:38.333086Z",
     "shell.execute_reply.started": "2021-12-10T04:05:38.315690Z"
    }
   },
   "outputs": [],
   "source": [
    "sub\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
